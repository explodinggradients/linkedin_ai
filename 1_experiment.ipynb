{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Guide\n",
    "\n",
    "### Prerequisites\n",
    "1. Create an account on [Ragas app](https://dev.app.ragas.io/)\n",
    "2. Obtain your [App TOKEN](https://dev.app.ragas.io/dashboard/settings/app-tokens) from the dashboard\n",
    "3. Have or create an [OpenAI API KEY](https://openai.com/)\n",
    "\n",
    "### Setting Up Observability\n",
    "\n",
    "1. Navigate to the linkedin_ai directory:\n",
    "   ```bash\n",
    "   cd linkedin_ai\n",
    "   ```\n",
    "\n",
    "2. Host mlflow UI and point to the URI\n",
    "    ```bash\n",
    "    mlflow ui --port 8080 --backend-store-uri file:///<your-file-path>/linkedin_ai/mlruns    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = \"\"\n",
    "os.environ[\"RAGAS_API_BASE_URL\"] = \"https://api.dev.app.ragas.io\"\n",
    "os.environ[\"OPENAI_API_KEY\"] =  \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "tracking_uri = os.path.join(current_dir, \"mlruns\")\n",
    "tracking_uri = f\"file:///{tracking_uri.lstrip('/')}\"\n",
    "\n",
    "os.environ[\"MLFLOW_HOST\"] = \"http://127.0.0.1:8080\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = tracking_uri\n",
    "mlflow.set_tracking_uri(tracking_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/shahules/Myprojects/linkedin_ai/mlruns/481371020783987734', creation_time=1744920240094, experiment_id='481371020783987734', last_update_time=1744920240094, lifecycle_stage='active', name='my_hackathon_experiment', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow uses this to log your traces\n",
    "mlflow.create_experiment(\n",
    "    \"my_hackathon_experiment\",\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/shahules/Myprojects/linkedin_ai/mlruns/481371020783987734', creation_time=1744920240094, experiment_id='481371020783987734', last_update_time=1744920240094, lifecycle_stage='active', name='my_hackathon_experiment', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\n",
    "    \"my_hackathon_experiment\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your linkedin_ai endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 437 LinkedIn posts\n",
      "BM25 index initialized\n"
     ]
    }
   ],
   "source": [
    "from linkedin_ai import LinkedinAI\n",
    "my_ai = await LinkedinAI.from_bm25('data/posts.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meta's open-source approach, particularly with models like LLAMA, is something I support. Open-sourcing models can foster innovation and collaboration within the AI community. It allows researchers and developers to build upon existing work, accelerating progress and enabling a broader range of applications. This approach aligns with the vision of creating AI systems that can understand and interact with the physical world, ultimately benefiting society as a whole.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await my_ai.ask(\"what is your take on opensource models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read hackathon test dataset from Ragas App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import BaseModel\n",
    "\n",
    "class TestDataset(BaseModel):\n",
    "    question: str\n",
    "    citations: str\n",
    "    grading_notes: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import Project\n",
    "\n",
    "p = Project.get(name=\"yann-lecun-wisdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = p.get_dataset(dataset_name=\"test-yann-lecun\", model=TestDataset)\n",
    "test_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestDataset(question='Where did you do your PhD and what was your field of study?', citations='[\"https://www.linkedin.com/in/yann-lecun\"]', grading_notes='- Specifies Pierre and Marie Curie University.\\n- Field of study should be computer science.\\n- Bonus if year range (1983–1987) is included.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fail'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# de\n",
    "from ragas_experimental.llm import ragas_llm\n",
    "from ragas_experimental.metric import DiscreteMetric\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=AsyncOpenAI())\n",
    "\n",
    "my_metric = DiscreteMetric(\n",
    "    llm=llm,\n",
    "    name='correctness',\n",
    "    prompt=\"Given the Question: {query} \\n Evaluate if given answer {response} \\n based on the Grading notes\\n: {grading_notes}.\",\n",
    "    values=[\"pass\",\"fail\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# test LLM as judge\n",
    "result = my_metric.score(query=\"what is your response\", response=\"this is my response\",grading_notes=\"- response should not contains word response\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your first experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.tracing.mlflow import sync_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "import ragas_experimental.typing as rt\n",
    "\n",
    "class ExperimentModel(TestDataset):\n",
    "    response: str\n",
    "    score: t.Annotated[t.Literal[\"pass\",\"fail\"], rt.Select(colors=[\"green\",\"red\"])]\n",
    "    score_reason: str\n",
    "    trace_url:  t.Annotated[str, rt.Url()]\n",
    "    \n",
    "\n",
    "@p.mlflow_experiment(ExperimentModel)\n",
    "async def experiment_func(item: TestDataset):\n",
    "    response = await my_ai.ask(item.question)\n",
    "    trace = await sync_trace()\n",
    "    score = await my_metric.ascore(query=item.question, response=response, grading_notes=item.grading_notes)\n",
    "    return ExperimentModel(question=item.question, citations=item.citations, grading_notes=item.grading_notes, response=response, score=score.result, score_reason=score.reason,trace_url = trace.get_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiment: 100%|██████████| 100/100 [00:36<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging changes to tracked files\n",
      "Changes committed with hash: 8d1c8d8e\n",
      "Created branch: ragas/stoic_adleman\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(name=stoic_adleman, model=ExperimentModel)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await experiment_func.run_async(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Experiments tab in Ragas app and view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your next experiment\n",
    "- Make a change in your app code/ endpoint\n",
    "- Here I am changing how many chunks I retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ai = await LinkedinAI.from_bm25('data/posts.json',top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run another experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiment: 100%|██████████| 100/100 [00:44<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging changes to tracked files\n",
      "Changes committed with hash: a0692b82\n",
      "Created branch: ragas/elegant_rivest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(name=elegant_rivest, model=ExperimentModel)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await experiment_func.run_async(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Experiments tab in Ragas app and view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Experiments\n",
    "- Go to Experiments tab in Ragas app and view different experiments\n",
    "- Compare the experiments and see what works best for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### LLM as judge giving incorrect results?\n",
    "- Train it. \n",
    "\n",
    "1. Review and change incorrect results in Experiments tab\n",
    "2. optimize llm as judge from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.embedding import ragas_embedding\n",
    "\n",
    "from openai import OpenAI\n",
    "embedding = ragas_embedding(provider='openai',client=OpenAI(),model=\"text-embedding-3-small\")\n",
    "my_metric.train(p,experiment_names=['<your-experiment-name-here>'],embedding_model=embedding,model=ExperimentModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
