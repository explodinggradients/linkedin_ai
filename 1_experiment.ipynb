{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Guide\n",
    "\n",
    "### Prerequisites\n",
    "1. Create an account on [Ragas app](https://beta.app.ragas.io/)\n",
    "2. Obtain your [App TOKEN](https://dev.app.ragas.io/dashboard/settings/app-tokens) from the dashboard\n",
    "3. Have or create an [OpenAI API KEY](https://openai.com/)\n",
    "\n",
    "### Setting Up Observability\n",
    "\n",
    "We use MLFlow for tracing. MLflow has already been installed but you have to start the server locally with `mlflow ui` command.\n",
    "\n",
    "1. Navigate to the linkedin_ai directory:\n",
    "   ```bash\n",
    "   cd linkedin_ai\n",
    "   ```\n",
    "\n",
    "2. Host mlflow UI and point to the URI\n",
    "    ```bash\n",
    "    mlflow ui --port 8080 --backend-store-uri file:///<your-file-path>/linkedin_ai/mlruns    \n",
    "    ```\n",
    "\n",
    "to find the exact command to run check üëáüèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to find the mlflow path\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "tracking_uri = os.path.join(current_dir, \"mlruns\")\n",
    "tracking_uri = f\"file:///{tracking_uri.lstrip('/')}\"\n",
    "\n",
    "print(\"now you run this in your terminal\")\n",
    "print(f\"$kmlflow ui --port 8080 --backend-store-uri {tracking_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = \"\"\n",
    "os.environ[\"RAGAS_API_BASE_URL\"] = \"https://api.dev.app.ragas.io\"\n",
    "os.environ[\"OPENAI_API_KEY\"] =  \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "\n",
    "os.environ[\"MLFLOW_HOST\"] = \"http://127.0.0.1:8080\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = tracking_uri\n",
    "mlflow.set_tracking_uri(tracking_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow uses this to log your traces\n",
    "mlflow.create_experiment(\n",
    "    \"my_hackathon_experiment\",\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\n",
    "    \"my_hackathon_experiment\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your linkedin_ai endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_ai import LinkedinAI\n",
    "my_ai = await LinkedinAI.from_bm25('data/posts.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await my_ai.ask(\"what is your take on opensource models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read hackathon test dataset from Ragas App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import BaseModel\n",
    "\n",
    "class TestDataset(BaseModel):\n",
    "    question: str\n",
    "    citations: str\n",
    "    grading_notes: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import Project\n",
    "\n",
    "p = Project.get(name=\"yann-lecun-wisdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = p.get_dataset(dataset_name=\"test-yann-lecun\", model=TestDataset)\n",
    "test_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de\n",
    "from ragas_experimental.llm import ragas_llm\n",
    "from ragas_experimental.metric import DiscreteMetric\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=AsyncOpenAI())\n",
    "\n",
    "my_metric = DiscreteMetric(\n",
    "    llm=llm,\n",
    "    name='score',\n",
    "    prompt=\"Given the Question: {query} \\n Evaluate if given answer {response} \\n based on the Grading notes\\n: {grading_notes}.\",\n",
    "    values=[\"pass\",\"fail\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# test LLM as judge\n",
    "result = my_metric.score(query=\"what is your response\", response=\"this is my response\",grading_notes=\"- response should not contains word response\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your first experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.tracing.mlflow import sync_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "import ragas_experimental.typing as rt\n",
    "\n",
    "class ExperimentModel(TestDataset):\n",
    "    response: str\n",
    "    score: t.Annotated[t.Literal[\"pass\",\"fail\"], rt.Select(colors=[\"green\",\"red\"])]\n",
    "    score_reason: str\n",
    "    trace_url:  t.Annotated[str, rt.Url()]\n",
    "    \n",
    "\n",
    "@p.mlflow_experiment(ExperimentModel, save_to_git=False) # save_to_git allows you to save the experiment to the git repo as a commit\n",
    "async def experiment_func(item: TestDataset):\n",
    "    response = await my_ai.ask(item.question)\n",
    "    trace = await sync_trace()\n",
    "    score = await my_metric.ascore(query=item.question, response=response, grading_notes=item.grading_notes)\n",
    "    return ExperimentModel(question=item.question, citations=item.citations, grading_notes=item.grading_notes, response=response, score=score.result, score_reason=score.reason,trace_url = trace.get_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await experiment_func.run_async(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Experiments tab in Ragas app and view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your next experiment\n",
    "- Make a change in your app code/ endpoint\n",
    "- Here I am changing how many chunks I retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ai = await LinkedinAI.from_bm25('data/posts.json',top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run another experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await experiment_func.run_async(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Experiments tab in Ragas app and view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Experiments\n",
    "- Go to Experiments tab in Ragas app and view different experiments\n",
    "- Compare the experiments and see what works best for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### LLM as judge giving incorrect results?\n",
    "- Train it. \n",
    "\n",
    "1. Review and change incorrect results in Experiments tab\n",
    "2. optimize llm as judge from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.embedding import ragas_embedding\n",
    "\n",
    "from openai import OpenAI\n",
    "embedding = ragas_embedding(provider='openai',client=OpenAI(),model=\"text-embedding-3-small\")\n",
    "my_metric.train(p,experiment_names=['<your-experiment-name-here>'],embedding_model=embedding,model=ExperimentModel, method={})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
