{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Guide\n",
    "\n",
    "### Prerequisites\n",
    "1. Create an account on [Ragas app](https://dev.app.ragas.io/)\n",
    "2. Obtain your [App TOKEN](https://dev.app.ragas.io/dashboard/settings/app-tokens) from the dashboard\n",
    "3. Have or create an [OpenAI API KEY](https://openai.com/)\n",
    "\n",
    "### Setting Up Observability\n",
    "\n",
    "1. Navigate to the linkedin_ai directory:\n",
    "   ```bash\n",
    "   cd linkedin_ai\n",
    "   ```\n",
    "\n",
    "2. Host mlflow UI and point to the URI\n",
    "    ```bash\n",
    "    mlflow ui --port 8080 --backend-store-uri file:///<your-file-path>/linkedin_ai/mlruns    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = \"apt.4dd8-fdd20d34b8e7-02c0-bb31-ed6ed5bd-df106\"\n",
    "os.environ[\"RAGAS_API_BASE_URL\"] = \"https://api.dev.app.ragas.io\"\n",
    "#os.environ[\"OPENAI_API_KEY\"] =  \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "tracking_uri = os.path.join(current_dir, \"mlruns\")\n",
    "tracking_uri = f\"file:///{tracking_uri.lstrip('/')}\"\n",
    "\n",
    "os.environ[\"MLFLOW_HOST\"] = \"http://127.0.0.1:8080\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = tracking_uri\n",
    "mlflow.set_tracking_uri(tracking_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'905517997201736171'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow uses this to log your traces\n",
    "mlflow.create_experiment(\n",
    "    \"my_hackathon_experiment\",\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/jjmachan/workspace/eglabs/clientwork/linkedin_ai/mlruns/905517997201736171', creation_time=1744931730125, experiment_id='905517997201736171', last_update_time=1744931730125, lifecycle_stage='active', name='my_hackathon_experiment', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\n",
    "    \"my_hackathon_experiment\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your linkedin_ai endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 437 LinkedIn posts\n",
      "BM25 index initialized\n"
     ]
    }
   ],
   "source": [
    "from linkedin_ai import LinkedinAI\n",
    "my_ai = await LinkedinAI.from_bm25('data/posts.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Open-source models are a significant part of Meta's approach, as seen with LLAMA. I believe that open-sourcing models can accelerate innovation and collaboration within the AI community. By making models and code accessible, we enable researchers and developers to build upon existing work, fostering a more inclusive and rapid advancement in AI technologies. This approach aligns with the vision of creating AI systems that can better understand and interact with the physical world.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await my_ai.ask(\"what is your take on opensource models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read hackathon test dataset from Ragas App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import BaseModel\n",
    "\n",
    "class TestDataset(BaseModel):\n",
    "    question: str\n",
    "    citations: str\n",
    "    grading_notes: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import Project\n",
    "\n",
    "p = Project.get(name=\"yann-lecun-wisdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = p.get_dataset(dataset_name=\"test-yann-lecun\", model=TestDataset)\n",
    "test_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestDataset(question='Did you found or advise any startups in the AI or biometrics space?', citations='[\"https://www.linkedin.com/in/yann-lecun\"]', grading_notes='- Should mention Element Inc as a co-founded startup.\\n- May also mention Museami.\\n- Describes involvement in biometric authentication or AI-related products.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fail'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# de\n",
    "from ragas_experimental.llm import ragas_llm\n",
    "from ragas_experimental.metric import DiscreteMetric\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=AsyncOpenAI())\n",
    "\n",
    "my_metric = DiscreteMetric(\n",
    "    llm=llm,\n",
    "    name='correctness',\n",
    "    prompt=\"Given the Question: {query} \\n Evaluate if given answer {response} \\n based on the Grading notes\\n: {grading_notes}.\",\n",
    "    values=[\"pass\",\"fail\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# test LLM as judge\n",
    "result = my_metric.score(query=\"what is your response\", response=\"this is my response\",grading_notes=\"- response should not contains word response\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your first experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.tracing.mlflow import sync_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "import ragas_experimental.typing as rt\n",
    "\n",
    "class ExperimentModel(TestDataset):\n",
    "    response: str\n",
    "    score: t.Annotated[t.Literal[\"pass\",\"fail\"], rt.Select(colors=[\"green\",\"red\"])]\n",
    "    score_reason: str\n",
    "    trace_url:  t.Annotated[str, rt.Url()]\n",
    "    \n",
    "\n",
    "@p.mlflow_experiment(ExperimentModel, save_to_git=False) # save_to_git allows you to save the experiment to the git repo as a commit\n",
    "async def experiment_func(item: TestDataset):\n",
    "    response = await my_ai.ask(item.question)\n",
    "    trace = await sync_trace()\n",
    "    score = await my_metric.ascore(query=item.question, response=response, grading_notes=item.grading_notes)\n",
    "    return ExperimentModel(question=item.question, citations=item.citations, grading_notes=item.grading_notes, response=response, score=score.result, score_reason=score.reason,trace_url = trace.get_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "p.mlflow_experiment(\n",
      "    experiment_model,\n",
      "    name_prefix: str = \u001b[33m''\u001b[39m,\n",
      "    save_to_git: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    stage_all: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Decorator for creating experiment functions with mlflow integration.\n",
      "\n",
      "Args:\n",
      "    experiment_model: The NotionModel type to use for experiment results\n",
      "    name_prefix: Optional prefix for experiment names\n",
      "\n",
      "Returns:\n",
      "    Decorator function that wraps experiment functions with mlflow observation\n",
      "\u001b[31mFile:\u001b[39m      ~/workspace/eglabs/clientwork/linkedin_ai/.venv/lib/python3.12/site-packages/ragas_experimental/project/experiments.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "p.mlflow_experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiment: 100%|██████████| 100/100 [00:46<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes detected, nothing to commit\n",
      "Created branch: ragas/quirky_shamir\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(name=quirky_shamir, model=ExperimentModel)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await experiment_func.run_async(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Experiments tab in Ragas app and view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your next experiment\n",
    "- Make a change in your app code/ endpoint\n",
    "- Here I am changing how many chunks I retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ai = await LinkedinAI.from_bm25('data/posts.json',top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run another experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiment: 100%|██████████| 100/100 [00:44<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging changes to tracked files\n",
      "Changes committed with hash: a0692b82\n",
      "Created branch: ragas/elegant_rivest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(name=elegant_rivest, model=ExperimentModel)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await experiment_func.run_async(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Experiments tab in Ragas app and view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Experiments\n",
    "- Go to Experiments tab in Ragas app and view different experiments\n",
    "- Compare the experiments and see what works best for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "### LLM as judge giving incorrect results?\n",
    "- Train it. \n",
    "\n",
    "1. Review and change incorrect results in Experiments tab\n",
    "2. optimize llm as judge from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.embedding import ragas_embedding\n",
    "\n",
    "from openai import OpenAI\n",
    "embedding = ragas_embedding(provider='openai',client=OpenAI(),model=\"text-embedding-3-small\")\n",
    "my_metric.train(p,experiment_names=['<your-experiment-name-here>'],embedding_model=embedding,model=ExperimentModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
