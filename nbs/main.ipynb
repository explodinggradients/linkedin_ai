{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "> Main class for Linkedin_AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List\n",
    "import aiofiles\n",
    "from openai import AsyncOpenAI\n",
    "from mlflow import trace\n",
    "\n",
    "from linkedin_ai.document import Document\n",
    "from linkedin_ai.retrievers import BM25Retriever, VectorRetriever, BaseRetriever\n",
    "\n",
    "\n",
    "\n",
    "class LinkedinAI:\n",
    "    \"\"\"Main RAG system for LinkedIn posts with factory methods for different retrieval strategies.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        retriever: BaseRetriever,\n",
    "        model: str = \"gpt-4o\",\n",
    "        max_tokens: int = 1000,\n",
    "        temperature: float = 0.1,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        self.retriever = retriever\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.client = AsyncOpenAI()\n",
    "    \n",
    "    @classmethod\n",
    "    async def _load_posts(cls, file_path: str) -> List[Document]:\n",
    "        \"\"\"Load LinkedIn posts from a JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Data file {file_path} not found\")\n",
    "        \n",
    "        documents = []\n",
    "        try:\n",
    "            async with aiofiles.open(file_path, mode='r', encoding='utf-8') as file:\n",
    "                content = await file.read()\n",
    "                posts_data = json.loads(content)\n",
    "                \n",
    "            for post_id, post_info in posts_data.items():\n",
    "                doc = Document(\n",
    "                    id=post_id,\n",
    "                    content=post_info.get(\"content\", \"\"),\n",
    "                    url=post_info.get(\"url\", \"\"),\n",
    "                    date=post_info.get(\"date\", \"unknown\")\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                \n",
    "            print(f\"Loaded {len(documents)} LinkedIn posts\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading data: {e}\")\n",
    "    \n",
    "    @classmethod\n",
    "    async def from_bm25(\n",
    "        cls,\n",
    "        posts: str,\n",
    "        model: str = \"gpt-4o\",\n",
    "        top_k: int = 3,\n",
    "        max_tokens: int = 1000,\n",
    "        temperature: float = 0.1,\n",
    "        verbose: bool = False\n",
    "    ) -> \"LinkedinAI\":\n",
    "        \"\"\"Create a LinkedinAI instance using BM25 retrieval.\"\"\"\n",
    "\n",
    "        # Load documents\n",
    "        documents = await cls._load_posts(posts)\n",
    "        \n",
    "        # Create and initialize retriever\n",
    "        retriever = BM25Retriever(documents, top_k)\n",
    "        await retriever.initialize()\n",
    "        \n",
    "        return cls(retriever,model, max_tokens, temperature, verbose)\n",
    "    \n",
    "    @classmethod\n",
    "    async def from_vector_search(\n",
    "        cls,\n",
    "        posts: str,\n",
    "        embedding_model: str = \"text-embedding-ada-002\",\n",
    "        model: str = \"gpt-4o\",\n",
    "        top_k: int = 3,\n",
    "        max_tokens: int = 1000,\n",
    "        temperature: float = 0.1,\n",
    "        verbose: bool = False\n",
    "    ) -> \"LinkedinAI\":\n",
    "        \"\"\"Create a LinkedinAI instance using vector search retrieval.\"\"\"\n",
    "        # Initialize OpenAI client with Instructor\n",
    "        \n",
    "        # Load documents\n",
    "        documents = await cls._load_posts(posts)\n",
    "        \n",
    "        # Create vector file path\n",
    "        vector_file = posts.replace(\".json\", \"_vectors.npy\")\n",
    "        \n",
    "        # Create and initialize retriever\n",
    "        retriever = VectorRetriever(embedding_model, documents, top_k)\n",
    "        # Pass the client during initialization\n",
    "        await retriever.initialize(vector_file)\n",
    "        \n",
    "        return cls(retriever, model, max_tokens, temperature, verbose)\n",
    "    \n",
    "    @trace()\n",
    "    async def ask(self, query: str, verbose: bool = False) -> str:\n",
    "        \"\"\"Answer a query using the RAG system.\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Query: {query}\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = await self.retriever.retrieve(query)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Retrieved {len(relevant_docs)} documents\")\n",
    "            for i, doc in enumerate(relevant_docs):\n",
    "                print(f\"Document {i+1}: {doc}\")\n",
    "        \n",
    "        # Format context from retrieved documents\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Post {i+1} (Date: {doc.date}):\\n{doc.content}\\nURL: {doc.url}\"\n",
    "            for i, doc in enumerate(relevant_docs)\n",
    "        ])\n",
    "        \n",
    "        # Generate answer using instructor and Pydantic model\n",
    "        system_message = \"You are a helpful assistant that provides information based on given context. Do not add any additional information not present in the context.\"\n",
    "        user_message = f\"Here is the context:\\n\\n{context}\\n\\nBased on this information, please answer: {query}\"\n",
    "        \n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=self.temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = {\n",
    "  \"7316949721615405057\": {\n",
    "    \"content\": \"Memory Networks and its descendants deserve more attention (so to speak ðŸ˜‚).\",\n",
    "    \"url\": \"https://www.linkedin.com/posts/yann-lecun_at-the-10th-year-anniversary-of-our-memory-activity-7316949721615405057-GdGw?utm_source=share&utm_medium=member_desktop&rcm=ACoAACHEQ1kBau5gFVkfSgsSB2flft8HtbfWS74\",\n",
    "    \"date\": \"1h\"\n",
    "  },\n",
    "  \"7316885912183857152\": {\n",
    "    \"content\": \"Just six weeks in Meta working on Llamaâ€™s multimodal capabilities, and itâ€™s been all about moving fast. I have already contributed to Llama4â€™s image grounding capabilities.\\n\\nLlama4 Scout is the best-in-class on image grounding, able to align user prompts with relevant visual concepts and anchor model responses to regions in the image. This enables more precise visual question answering for the LLM to better understand user intent and localize objects of interest. \\n\\nI am super thrilled that the grounding capabilities got featured as a demo in the Llama blog post (link below)! Many thanks to the awesome teammates for the opportunities and the support.\\n\\nLink to LLAMA-4: https://www.llama.com/\",\n",
    "    \"url\": \"https://www.linkedin.com/posts/yash-patel-93626945_just-six-weeks-in-meta-working-on-llamas-ugcPost-7314834583294787584-srbb?utm_source=share&utm_medium=member_desktop&rcm=ACoAACHEQ1kBau5gFVkfSgsSB2flft8HtbfWS74\",\n",
    "    \"date\": \"5d\"\n",
    "  },\n",
    "  \"7316885428165373952\": {\n",
    "    \"content\": \"On April 3, the VinFuture Foundation successfully orchestrated the final \\\"Call for Nominations\\\" webinar in preparation for the 2025 Award season. It meticulously provided comprehensive guidelines for nomination partners and scientists interested in the Prize. The webinar was attended by over 120 scientists and experts from 26 countries worldwide.\\n\\nThe webinar was chaired by the esteemed innovator Professor Thuc-Quyen Nguyen, Chair of the VinFuture Prize Pre-Screening Committee. Co-chairing the session was Professor Martin Andrew Green, a Member of the VinFuture Prize Council and Laureate of the 2023 VinFuture Prize. Together, they disseminated crucial insights regarding the nomination procedures and encouraged robust participation from the scientific community. Notably, the presence of Professor Yann LeCun, a 2024 VinFuture Grand Prize Laureate recognized for his significant contributions to the AI revolution, served as a profound source of inspiration for researchers and nomination partners to embrace and honor future technological advancements.\",\n",
    "    \"url\": \"https://www.linkedin.com/posts/vinfuture-prize_on-april-3-the-vinfuture-foundation-successfully-ugcPost-7315269860001755138-qtJo?utm_source=share&utm_medium=member_desktop&rcm=ACoAACHEQ1kBau5gFVkfSgsSB2flft8HtbfWS74\",\n",
    "    \"date\": \"4d\"\n",
    "  },\n",
    "}\n",
    "\n",
    "with open(\"posts.json\", \"w\") as f:\n",
    "    json.dump(posts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 LinkedIn posts\n",
      "BM25 index initialized\n",
      "\n",
      "=== Using Vector Search retrieval ===\n",
      "Loaded 3 LinkedIn posts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The context provided does not contain specific information on how to evaluate AI systems. It includes details about a webinar for the VinFuture Prize, work on Llama4's image grounding capabilities, and a mention of Memory Networks, but it does not discuss evaluation methods for AI systems.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_rag = await LinkedinAI.from_bm25(\n",
    "    posts=\"posts.json\",\n",
    "    model=\"gpt-4o\",\n",
    "    top_k=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "await my_rag.ask(\"What are your thoughts on RAG?\")\n",
    "\n",
    "\n",
    "\n",
    "# Example with Vector Search\n",
    "print(\"\\n=== Using Vector Search retrieval ===\")\n",
    "my_rag = await LinkedinAI.from_vector_search(\n",
    "    posts=\"posts.json\",\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    model=\"gpt-4o\",\n",
    "    top_k=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "await my_rag.ask(\"How do you evaluate AI systems?\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
