{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn AI: Running Experiments\n",
    "\n",
    "This notebook will guide you through setting up and running experiments to evaluate your LinkedIn AI assistant. \n",
    "\n",
    "You'll learn, how to:\n",
    "- Create test datasets\n",
    "- Define evaluation metrics using LLMs as judges\n",
    "- Run experiments to measure your AI's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install the Experimental RAGAS Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/explodinggradients/ragas_annotator.git@ragas-experimental -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Test Data\n",
    "\n",
    "Let's load a test dataset that contains questions, expected citations, and grading notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "\n",
    "# Load the test dataset from JSON\n",
    "with open(\"yann-lecun-wisdom/yann_test.json\", \"r\") as f:\n",
    "    data = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import BaseModel\n",
    "\n",
    "\n",
    "class TestDataset(BaseModel):\n",
    "    question: str\n",
    "    citations: list[str]\n",
    "    grading_notes: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure the Experiment Environment\n",
    "\n",
    "Visit `dev.app.ragas.io` to get your API Keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# go to dev.app.ragas.io to create an app token\n",
    "RAGAS_APP_TOKEN = \"your-app-token\"\n",
    "RAGAS_API_BASE_URL = \"https://api.dev.app.ragas.io\"\n",
    "\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = RAGAS_APP_TOKEN\n",
<<<<<<< HEAD
    "os.environ[\"RAGAS_API_BASE_URL\"] = RAGAS_API_BASE_URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/explodinggradients/ragas_annotator.git@ragas-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Project.create(\n",
    "    name=\"yann-lecun-wisdom\",\n",
    "    description=\"Yann LeCun Wisdom\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(name='yann-lecun-wisdom')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Project.get(name=\"yann-lecun-wisdom\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name=test-yann-lecun, model=TestDataset, len=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = p.create_dataset(name=\"test-yann-lecun\", model=TestDataset)\n",
    "for item in data:\n",
    "    t = TestDataset(question=item[\"question\"], citations=item[\"citations\"], grading_notes=item[\"grading_notes\"])\n",
    "    test_dataset.append(t)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = p.get_dataset(dataset_name=\"test-yann-lecun\", model=TestDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.load()"
=======
    "os.environ[\"RAGAS_API_BASE_URL\"] = RAGAS_API_BASE_URL"
>>>>>>> a712d7d (Improved docs and example notebooks)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a Project and Dataset\n",
    "\n",
    "Now let's set up our evaluation project and upload our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import Project\n",
    "\n",
    "# The Project.create() method sets up a new project\n",
    "\n",
    "project = Project.create(\n",
    "    name=\"yann-lecun-wisdom\",\n",
    "    description=\"Yann LeCun Wisdom\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(name='yann-lecun-wisdom-01')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = Project.get(name=\"yann-lecun-wisdom\")\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create and Populate the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name=test-yann-lecun, model=TestDataset, len=30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = project.create_dataset(name=\"test-yann-lecun\", model=TestDataset)\n",
    "\n",
    "for item in data:\n",
    "    t = TestDataset(\n",
    "        question=item[\"question\"],\n",
    "        citations=item[\"citations\"],\n",
    "        grading_notes=item[\"grading_notes\"],\n",
    "    )\n",
    "    test_dataset.append(t)\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = project.get_dataset(dataset_name=\"test-yann-lecun\", model=TestDataset)\n",
    "test_dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create an LLM-Based Evaluation Metric\n",
    "\n",
    "Let's define how we'll evaluate the quality of responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.llm import ragas_llm\n",
    "from ragas_experimental.metric import DiscreteMetric\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\", model=\"gpt-4o\", client=AsyncOpenAI())\n",
    "\n",
    "my_metric = DiscreteMetric(\n",
    "    llm=llm,\n",
    "    name=\"correctness\",\n",
    "    prompt=\"Given the Question: {query} \\n Evaluate if given answer {response} \\n based on the Grading notes\\n: {grading_notes}.\",\n",
    "    values=[\"pass\", \"fail\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test our evaluation metric with a simple example:\n",
    "\n",
    "- The grading note states the response shouldn't contain the word \"response\"\n",
    "- Since our test response does contain that word, the metric should return \"fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fail'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test LLM as judge\n",
    "result = my_metric.score(\n",
    "    query=\"what is your response\",\n",
    "    response=\"this is my response\",\n",
    "    grading_notes=\"- response should not contains word 'response'\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our evaluation system is working as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Initialize the LinkedIn AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 17:24:38 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 437 LinkedIn posts\n",
      "BM25 index initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"My response is centered around the importance of open access and open-source models in AI. I believe that as AI becomes an integral part of our interactions with the digital world, it is crucial for these systems to be open and accessible, much like the software infrastructure of the Internet. This openness ensures that AI systems can be a common infrastructure containing all human culture and knowledge, and their development should be crowd-sourced, similar to how Wikipedia operates.\\n\\nIn a panel discussion at the Paris Peace Forum, I emphasized that AI assistants will eventually become smarter than us, and thus, they need to be open-source to ensure transparency and trust. This is one reason why Meta made Llama-2 open and free. I also addressed a misconception about open access, highlighting that it is not about whether a company is for-profit or nonprofit, but about the openness and accessibility of the technology itself.\\n\\nFurthermore, in Meta's official response to the NTIA, we underscored the importance of open foundation models to U.S. economic, national security, and foreign policy interests. This aligns with my belief that open models are crucial for the advancement and ethical deployment of AI technologies.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from linkedin_ai import LinkedinAI\n",
    "\n",
    "my_ai = await LinkedinAI.from_bm25(\"yann-lecun-wisdom/yann-lecun_posts.json\")\n",
    "\n",
    "await my_ai.ask(\"what is your response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Create and Run the Experiment\n",
    "\n",
    "We're defining an experiment function decorated with @project.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentModel(TestDataset):\n",
    "    response: str\n",
    "    score: str\n",
    "    score_reason: str\n",
    "\n",
    "@project.experiment(ExperimentModel)\n",
    "async def experiment_func(item: TestDataset):\n",
    "    response = await my_ai.ask(item.question)\n",
    "    score = await my_metric.ascore(\n",
    "        query=item.question, response=response, grading_notes=item.grading_notes\n",
    "    )\n",
    "    return ExperimentModel(\n",
    "        question=item.question,\n",
    "        citations=item.citations,\n",
    "        grading_notes=item.grading_notes,\n",
    "        response=response,\n",
    "        score=score.result,\n",
    "        score_reason=score.reason,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:44<00:00,  1.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(name=hungry_postel, model=ExperimentModel)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await experiment_func.run_async(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Compare and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment(name=reverent_minsky, model=ExperimentModel)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.compare_and_plot(experiment_names=[], model=ExperimentModel, metric_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Now that you've run your first experiment, try:\n",
    "\n",
    "-\n",
    "- \n",
    "- \n",
    "\n",
    "Happy experimenting! ðŸ§ª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datadog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
